---
title: "Practical: ordination with other packages"
subtitle: "Physalia workshop on GLLVMs"
author: "Bert van der Veen"
output: html_document
---

# Description

On this last day of the workshop, I will demonstrate the use of a few other R packages for applying GLLVMs to fit JSDMs or model-based ordination. Most of these are slower in fitting than the <tt>gllvm</tt> R-package, or provide considerably fewer tools. However, that is not to say they are not useful: there are things that are currently difficult to do in the <tt>gllvm</tt> package that are easier to implement with these other R-packages. Either way, this provides you with some reference material on what <tt>gllvm</tt> does so well!

# R packages

We will briefly look at some of the R-packages that were also in my presentation. This includes:

- <tt>boral</tt>
- <tt>Hmsc</tt>
- <tt>ecoCopula</tt>
- <tt>glmmTMB</tt>
- <tt>gmf</tt>
- <tt>RCM</tt>
- <tt>latentINLA</tt>
- <tt>CBFM</tt>

Although there are probably more packages than these (let me know if you are missing some).

# Data

I collected some datasets that we can work with, but if you have your own data you can start by analyzing that instead. <tt>mvabund</tt> has a few datasets included that we can play with:

1. "tasmania": abundances of Copepod and Nematode species in a blocked design under a disturbance treatment
2. "antTraits": abundance of 41 ant species, with environmental data
3. "solberg": abundance of benthic invertebrate species with a variable of organic enrichment
4. "spider": abundance of 12 wolf spider species with environmental variables
5. "tikus":  abundance of coral species over time

and there are more datasets (e.g., "dune", "pyrifos", "mite", and "BCI") in the <tt>vegan</tt> R-package. These are even more datasets in the "data" folder of the workshop that you can use for this practical:

1. Beetles (abundance)
2. Birds (abundance)
3. wadden (abundance)
4. wadden (biomass)
4. fungi (presence-absence)
5. eucalypt (presence-absence)

My suggestion is to try a few different data types in this exercise (e.g., presence-absence (alpine, eucalypt), ordinal (dune, Skabbholmen), abundance (take your pick), biomass (wadden) to get an impression of what it takes to analyse such datatypes. For some of these response types (e.g., ordinal, biomass) it might be more difficult to find software for multispecies modeling that also supports a suitable response distribution (don't worry, <tt>gllvm</tt> has it all).

# Tasks

I will take a (very) small dataset, because it might otherwise take too long to try some of these packages. The (reduced) spider data is included in the <tt>mvabund</tt> package, and it contains observations of 12 species at 28 sites. There is an extended version of the dataset in <tt>gllvm</tt> available that contains 100 sites, but the environmental covariates were only observed at these 28.

```{r data}
data(spider, package = "mvabund")
Y <- spider$abund;row.names(Y) <- 1:nrow(Y)
X <- scale(spider$x)
```

## boral

The <tt>boral</tt> package with main function `boral` fits GLLVMs using JAGS; i.e., it is based on the Bayesian statistics paradigm and fits with MCMC. MCMC is generally (very) slow, so model fitting can take a while:

```{r boral1, cache = TRUE, message = FALSE}
library(boral)
model1 <- boral::boral(Y, X, formula.X = ~ soil.dry + moss + fallen.leaves, lv.control=list(num.lv = 2), family = "poisson", save.model = TRUE)
```

As in the packages with numerical optimisation, we need to be careful when it comes to convergence with MCMC. The main difference, is that with MCMC we need to inspect convergence of each parameter visually:

```{r boralmcmc, cache = TRUE, eval = FALSE}
coda::traceplot(boral::get.mcmcsamples(model1))
```
If the model has converged, the plot should look stationary, and the MCMC chain (black lines) should have explored the whole range of the y-axis. Boral fortunately also returns a statistic that can help us assess convergence more quantitatively. At the end of this list of statistics there are two values. The one under "TRUE" should be as large as possible (i.e., close to one). If the value is low, we need to fit the model longer (which I will not do here).

```{r boral2}
model1$geweke.diag
```

When the model has converged, we can inspect some of the results:

```{r boral3, echo = FALSE, message = FALSE}
library(boral)
lvsplot(model1) # ordination plot
coefsplot("fallen.leaves", model1) # caterpillar plot
calc.varpart(model1) # variance partitioning in boral
```

## Hierarchical modeling of species communities (HMSC)

HMSC is currently the most popular package for fitting Joint Species Distribution Models in the Bayesian paradigm - it focuses on fitting models to presence-absence data but there are a few count distributions supported too. Here is code for fitting a basic GLLVM:

```{r hmsc, cache = TRUE, message = FALSE}
library(Hmsc)
# need to set-up LVs
studyDesign = data.frame(sample=as.factor(1:nrow(Y)))
rL <- Hmsc::HmscRandomLevel(units = studyDesign$sample)
model2 <- Hmsc::Hmsc(Y, XFormula = ~fallen.leaves+soil.dry, XData= data.frame(X),
distr = "lognormal poisson", studyDesign = studyDesign, 
ranLevels = list(sample = rL))
# Run mcmc
run =  Hmsc::sampleMcmc(model2, samples = 1000, nChains = 3, 
              transient = 2500)
# make biplot
etaPost=Hmsc::getPostEstimate(run, "Eta")
lambdaPost=Hmsc::getPostEstimate(run, "Lambda")
Hmsc::biPlot(run, etaPost = etaPost, lambdaPost = lambdaPost, factors = c(1,2))
betaPost=Hmsc::getPostEstimate(run, "Beta")
plotBeta(model2, post = betaPost) # heatmap of covariate coefficients
```

Note that there is much more that Hmsc can do than I have shown here: you can fit fourth-corner models, with Phylogenetic rando effects as well, add additional random effects, specify the plot/sample level at which the latent variables are estimated, and incorporate spatial/temporal autocorrelation in the models. The package has a good set of vignettes that can help you figure out how to specify models.

Variance partitioning can be done with the following code.

```{r varpart, warning=FALSE, eval = FALSE}
VP = computeVariancePartitioning(run)
par(mar=c(4,4,4,4))
plotVariancePartitioning(model2, VP = VP,
                         las = 2, horiz=F)
```


## ecoCopula

The ecoCopula package is presently quite limited in its functionality, but it can fit unconstrainded ordinations -very- rapidly.

```{r eco, cache= TRUE, message= FALSE}
library(ecoCopula)
preModel <- stackedsdm(Y, formula_X =~fallen.leaves+moss+soil.dry, data = X)
model3 <- cord(preModel)
plot(model3, biplot=TRUE)
```

## Vector Generalized and Additive Models

The VGAM package can be used for all kinds of analyses, but for multivariate analysis in particular it can be used for unconstrained, constrained, and doubly constrained ordinations, with or without unimodal response model (e.g., as in the `cqo` function. The implementation is even more unstable than in <tt>gllvm</tt>.), and for vector GLMs as in <tt>mvabund</tt>.

The following code implements an unconstrained ordination:

```{r VGAMuo, cache = TRUE, message=FALSE}
library(VGAM)
# unconstrained (fixed effects) ordination
model4 <- rcim(Y, Rank = 2, family = poissonff)
lvplot(model4) # this plotting function is an acquired taste
```

A constrained ordination:

```{r VGAMco, cache = TRUE, message=FALSE, echo = -1}
library(VGAM)
# constrained (fixed effects) ordination
model5 <- rrvglm(Y ~ soil.dry+fallen.leaves+moss, data=data.frame(X),Rank = 2, family = poissonff)
lvplot(model5) # this plotting function is an acquired taste
```

A vector GLM:

```{r VGLM, cache = TRUE, echo  =-1, message=FALSE}
library(VGAM)
model6 <- vglm(Y ~ soil.dry+fallen.leaves+moss, data=data.frame(X), family = poissonff)
```

The VGAM package has so much functionality, it could use its own workshop.

## glmmTMB

This one we have also briefly covered on the first day: <tt>glmmTMB</tt> can be used for unconstrained ordination with random effects, and of course for general fitting of models with random effects and zero-inflated components. It does not have (much) functionality for presenting results, but there is support for a few additional R-packages that can help with that.

We need this code from [the glmmTMB github](https://github.com/glmmTMB/glmmTMB/issues/1012) to extract site and species scores:

```{r glmmtmb1}
extract_rr <- function(object){
  listname <- "cond"
  cnms <- object$modelInfo$reTrms[[listname]]$cnms   ## list of (named) terms and X columns
  flist <- object$modelInfo$reTrms[[listname]]$flist ## list of grouping variables
  flist_asgn <- attr(flist, "assign")
  levs <- lapply(seq_along(flist_asgn), function(i) levels(flist[[flist_asgn[i]]]))
  
  reStruc <- object$modelInfo$reStruc[[paste0(listname, "ReStruc")]] ## random-effects structure
  nc <- vapply(reStruc, function(x) x$blockSize, numeric(1)) ## number of RE params per block
  pl <- object$obj$env$parList(object$fit$par, object$fit$parfull)
  
  #function to split b by the random effect terms
  split.bseq <- function(object){
    listname <- "cond"
    reStruc <- object$modelInfo$reStruc[[paste0(listname, "ReStruc")]] ## random-effects structure
    nc <- vapply(reStruc, function(x) x$blockSize, numeric(1)) ## number of RE params per block
    nb <- vapply(reStruc, function(x) x$blockReps, numeric(1)) ## number of blocks per RE (may != nlevs in some cases)
    ### splitting the b's into their respective random effects
    nbseq <- rep.int(seq_along(nb), nb * nc)       ## splitting vector
    return(nbseq)
  }
  nbseq <- split.bseq(object)      ## splitting vector
  ml.b <- split(pl$b, nbseq)
  ml <- ml.b
  
  for (i in seq_along(ml.b)) {
    ml[[i]] <- matrix(ml.b[[i]], ncol = nc[i], byrow = TRUE,
                      dimnames = list(levs[[i]], cnms[[i]]))
  }
  
  get_rank <- function(x){
    if(x[["blockCode"]]==9){
      p <- x$blockSize
      nt <- x$blockNumTheta
      rank <- (2*p + 1 - sqrt((2*p+1)^2 - 8*nt))/2
    } else
      rank <- 0
    return(rank)
  }
  
  rank <- vapply(object$modelInfo$reStruc$condReStruc,
                 get_rank,
                 FUN.VALUE=numeric(1))
  nlv <- rank[rank > 0]
  rrName <- names(nlv)
  rrBlock <- which(rank > 0)
  b = ml[[rrBlock]][,1:nlv]
  colnames(b) <- paste0("lv", 1:nlv)
  fact_load <- object$obj$env$report(object$fit$parfull)$fact_load[[rrBlock]]
  rownames(fact_load) <- cnms[[rrBlock]]
  
  return(list(fl = fact_load, b = b))
}
```

We need to format our data into long format as shown in [the glmmTMB vignette](https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html#general-latent-variable-model):

```{r glmmtmb2}
## organize data into long format
sppTot <- sort(colSums(spider$abund), decreasing = TRUE)
tmp <- cbind(spider$abund, spider$x)
tmp$id <- 1:nrow(tmp)
spiderDat <- reshape(tmp,
                     idvar = "id",
                     timevar = "Species",
                     times =  colnames(spider$abund),
                     varying = list(colnames(spider$abund)),
                     v.names = "abund",
                     direction = "long")
```


Then we can construct an unconstrained ordination:

```{r glmmtmb3, cache = TRUE, message=FALSE}
library(glmmTMB)
model7 <- glmmTMB(abund ~ Species + Species:fallen.leaves + rr(Species + 0|id, d = 2),
                                             data = spiderDat)
rrstuf <- extract_rr(model7)
plot(rbind(rrstuf$b,rrstuf$fl), type = "n");
text(rrstuf$b);text(rrstuf$fl, col = "red")
```

## Generalized Matrix Factorization

The <tt>gmf</tt> package is not actively maintained, and it is also not on CRAN. It provides a different method for fitting unconstrained ordination GLLVMs, that is fast but can be very unstable.

```{r gmf, cache = TRUE, message=FALSE}
library(gmf)
# devtools::install_github("kidzik/gmf")
model8 <- gmf::gmf(Y, family = poisson(), p = 2)
plot(rbind(model8$u,model8$v), type = "n", xlab="LV1", ylab="LV2")
text(model8$u)
text(model8$v, col="red")
```

## RCM

The RCM package is geared towards analysis of (I think) microbial communities. It has some statistical issues also outlined in the associated article, and it (presently) only supports the negative-binomial distribution. In that case, you can use it to do unconstrained, constrained, and constrained additive ordination based on a fixed effects formulation. It requires the data to be formatted as a "phyloseq" object for constrained ordination.

Unconstrained ordination:

```{r RCMuo, cache=TRUE,message=FALSE}
# devtools::install_github("CenterForStatistics-UGent/RCM")
library(RCM)
library(phyloseq)
model9 <- RCM::RCM(Y, k = 2)
plot(model9)
```

Constrained ordination:

```{r RCMco,cache=TRUE,message=FALSE,echo=-1}
library(RCM)
library(phyloseq)
model10 <- RCM(phyloseq(otu_table(spider$abund, taxa_are_rows = FALSE), sample_data(spider$x)), covariates = c("soil.dry", "moss", "fallen.leaves"), k = 2)
plot(model10)
```
